{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUENTS AS WHOLE PLANS\n",
    "import os\n",
    "\n",
    "# Function to read all text files in a folder\n",
    "\n",
    "\n",
    "def read_text_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(\n",
    "                os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"files/txt\"\n",
    "texts = read_text_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUENTS AS INDIVIDUAL POLICIES\n",
    "\n",
    "from postgres import Postgres\n",
    "\n",
    "pg = Postgres()\n",
    "\n",
    "results = pg.query(\n",
    "    \"\"\"\n",
    " SELECT \n",
    "\tcmetadata->'text' as text\n",
    "    FROM langchain_pg_embedding \n",
    "\tWHERE cmetadata @> '{\"chunker\": \"sherpa\"}' \n",
    "    AND LENGTH(cmetadata->>'sections'::TEXT) > 20\n",
    "\tLIMIT 5000;\n",
    "                   \n",
    "\t\"\"\"\n",
    ")\n",
    "texts = [r[0] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def remove_place_names_and_stop_words(text):\n",
    "\n",
    "    custom_phrases = [\n",
    "        \"Abberley\",\n",
    "        \"Abbots Langley\",\n",
    "        \"Aberford\",\n",
    "        \"Ab Kettleby Parish\",\n",
    "        \"Acle\",\n",
    "        \"Acton\",\n",
    "        \"Addingham\",\n",
    "        \"Adel\",\n",
    "        \"Alcester\",\n",
    "        \"Alfold\",\n",
    "        \"Cleobury Mortimer\",\n",
    "        \"Corby Glen\",\n",
    "        \"Cossington\",\n",
    "        \"Elmswell\",\n",
    "        \"Fulbourn\",\n",
    "        \"Nether Whitacre\",\n",
    "        \"Sawtry\",\n",
    "        \"Strensall & Towthorpe\",\n",
    "        \"The Three Parishes\",\n",
    "        \"Totnes\",\n",
    "        \"West Wittering\",\n",
    "        \"Whaley Bridge\",\n",
    "        \"Winchfield\",\n",
    "        \"Winkfield\",\n",
    "    ]\n",
    "\n",
    "    doc = nlp(text)\n",
    "    # Remove place names and stop words\n",
    "    filtered_tokens = [\n",
    "        token.text for token in doc if token.ent_type_ != \"GPE\" and not token.is_stop\n",
    "    ]\n",
    "\n",
    "    # Reconstruct the filtered text\n",
    "    filtered_text = \" \".join(filtered_tokens)\n",
    "\n",
    "    # Remove custom stop phrases (case insensitive)\n",
    "    filtered_text_lower = filtered_text.lower()\n",
    "    for phrase in custom_phrases:\n",
    "        filtered_text_lower = filtered_text_lower.replace(phrase.lower(), \"\")\n",
    "\n",
    "    # Split and join to remove extra spaces\n",
    "    filtered_text = \" \".join(filtered_text_lower.split())\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "processed_texts = [remove_place_names_and_stop_words(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: lane, street, houses, church, north, photo, strensall, station, brick, parking\n",
      "Topic 1: parking, energy, amenity, scale, street, car, charging, impact, residential, electric\n",
      "Topic 2: town, transport, strategy, core, safe, peak, routes, way, network, improve\n",
      "Topic 3: employment, services, business, businesses, uses, opportunities, retail, health, town, economy\n",
      "Topic 4: environmental, water, walsham, flood, le, willows, risk, assessment, flooding, change\n",
      "Topic 5: school, canal, figure, primary, street, footpath, access, sinc, footpaths, network\n",
      "Topic 6: biodiversity, wildlife, trees, woodland, species, habitats, planting, nature, native, hedgerows\n",
      "Topic 7: uk, www, https, gov, np, org, authority, huntingdonshire, applications, pdf\n",
      "Topic 8: sports, walking, transport, club, leisure, facility, recreation, bus, cycling, hall\n",
      "Topic 9: century, west, house, ii, brick, church, north, east, roof, windows\n",
      "Topic 10: boundary, detached, significance, century, defined, building, photo, generally, north, trees\n",
      "Topic 11: heritage, assets, conservation, views, setting, view, listed, significance, asset, non\n",
      "Topic 12: traffic, transport, car, parking, cycle, services, pedestrian, street, cycling, shops\n",
      "Topic 13: nppf, settlement, plans, adopted, strategy, considered, strategic, meet, core, boundary\n",
      "Topic 14: strensall, common, cf2, towthorpe, residential, close, properties, worcestershire, photo, york\n",
      "Topic 15: consultation, group, survey, steering, 2022, 2018, draft, evidence, 2017, 2021\n",
      "Topic 16: cil, referendum, statutory, act, variety, developed, projects, levy, independent, examination\n",
      "Topic 17: objective, building, group, objectives, architectural, value, street, age, walsham, cottages\n",
      "Topic 18: homes, affordable, households, dwellings, population, 16, 15, 2011, census, 11\n",
      "Topic 19: value, yes, recreational, wildlife, north, play, adjacent, properties, east, basis\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_df=0.10, min_df=4, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Apply LDA\n",
    "n_topics = 20  # Number of topics\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Get the topics and words\n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [\n",
    "            feature_names[i] for i in topic.argsort()[: -no_top_words - 1: -1]\n",
    "        ]\n",
    "        topics[f\"Topic {topic_idx}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "\n",
    "no_top_words = 10\n",
    "topics = display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n",
    "\n",
    "# Display the topics\n",
    "for topic, words in topics.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

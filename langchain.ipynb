{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Scientists', 'Backend Developers', 'API Developers']\n",
      "START\n",
      "\n",
      "Data Scientists:\n",
      "\n",
      "Data Scientists have a deep understanding of data and how it should be processed, making them highly qualified to write efficient backend and API codes.\n",
      "Data Scientists:\n",
      "\n",
      "Data Scientists have the necessary analytical and problem-solving skills to write effective backend and API codes.\n",
      "Data Scientists:\n",
      "\n",
      "Data Scientists have a unique perspective and understanding of data that enables them to write more optimized backend and API codes.\n",
      "Backend Developers:\n",
      "\n",
      "Backend Developers have specialized knowledge and training in coding and software development, making them more qualified to write efficient and maintainable backend and API codes.\n",
      "Data Scientists:\n",
      "\n",
      "Data Scientists have the ability to understand and analyze complex data, making them well-equipped to write efficient and robust backend and API codes.\n",
      "Backend Developers:\n",
      "\n",
      "Backend Developers have extensive experience and expertise in writing code that is scalable, secure, and follows industry best practices.\n",
      "Data Scientists:\n",
      "\n",
      "Data Scientists have a deep understanding of data and how it should be processed, making them highly qualified to write efficient backend and API codes that are tailored specifically for data-driven applications.\n",
      "Backend Developers:\n",
      "\n",
      "Backend Developers have a specialized skill set that allows them to focus solely on writing efficient and maintainable backend and API codes, while Data Scientists have a broader focus on data analysis and modeling.\n",
      "Data Scientists:\n",
      "\n",
      "Data Scientists have a deep understanding of data and how it can be utilized, making them well-equipped to design and implement efficient backend and API codes.\n",
      "Backend Developers:\n",
      "\n",
      "Backend Developers have specialized training and expertise in building and maintaining complex backend systems, making them more qualified to write efficient and reliable backend and API codes.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "import random\n",
    "import time\n",
    "\n",
    "llm = OpenAI()\n",
    "debate_topic= \"Should Data Scientists write backend and API codes as well?\"\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output = llm(\"I wish to have a debate on {}. What would be the fighting sides called? Output just the names and nothing else as comma separated list\".format(debate_topic))\n",
    "classes = output_parser.parse(output)\n",
    "\n",
    "print(classes)\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    classification: Optional[str] = None\n",
    "    history: Optional[str] = None\n",
    "    current_response: Optional[str] = None\n",
    "    count: Optional[int]=None\n",
    "    results: Optional[str]=None\n",
    "    greeting: Optional[str]=None\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "prefix_start= '''You are in support of {} . You are in a debate with {} over the\n",
    " topic: {}. This is the conversation so far \\n{}\\n. \n",
    "Put forth your next argument to support {} countering {}.\\\n",
    "Dont repeat your previous arguments. Give a short, one line answer.'''\n",
    "\n",
    "def classify(question):\n",
    "    return llm(\"classify the sentiment of input as {} or {}. Output just the class. Input:{}\".format('_'.join(classes[0].split(' ')),'_'.join(classes[1].split(' ')),question)).strip()\n",
    "\n",
    "def classify_input_node(state):\n",
    "    question = state.get('current_response')\n",
    "    classification = classify(question)  # Assume a function that classifies the input\n",
    "    return {\"classification\": classification}\n",
    "\n",
    "def handle_greeting_node(state):\n",
    "    return {\"greeting\": \"Hello! Today we will witness the fight between {} vs {}\".format(classes[0],classes[1])}\n",
    "\n",
    "def handle_pro(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    if summary=='Nothing':\n",
    "        prompt = prefix_start.format(classes[0],classes[1],debate_topic,'Nothing',classes[0],\"Nothing\")\n",
    "        argument = classes[0] +\":\"+ llm(prompt)\n",
    "        summary = 'START\\n'\n",
    "    else:\n",
    "        prompt = prefix_start.format(classes[0],classes[1],debate_topic,summary,classes[0],current_response)\n",
    "        argument = classes[0] +\":\"+ llm(prompt)\n",
    "    return {\"history\":summary+'\\n'+argument,\"current_response\":argument,\"count\":state.get('count')+1}\n",
    "\n",
    "def handle_opp(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    prompt = prefix_start.format(classes[1],classes[0],debate_topic,summary,classes[1],current_response)\n",
    "    argument = classes[1] +\":\"+ llm(prompt)\n",
    "    return {\"history\":summary+'\\n'+argument,\"current_response\":argument,\"count\":state.get('count')+1}    \n",
    "\n",
    "def result(state):\n",
    "    summary = state.get('history').strip()\n",
    "    prompt = \"Summarize the conversation and judge who won the debate.No ties are allowed. Conversation:{}\".format(summary)\n",
    "    return {\"results\":llm(prompt)}\n",
    "\n",
    "workflow.add_node(\"classify_input\", classify_input_node)\n",
    "workflow.add_node(\"handle_greeting\", handle_greeting_node)\n",
    "workflow.add_node(\"handle_pro\", handle_pro)\n",
    "workflow.add_node(\"handle_opp\", handle_opp)\n",
    "workflow.add_node(\"result\", result)\n",
    "\n",
    "\n",
    "\n",
    "def decide_next_node(state):\n",
    "    return \"handle_opp\" if state.get('classification')=='_'.join(classes[0].split(' ')) else \"handle_pro\"\n",
    "\n",
    "def check_conv_length(state):\n",
    "    return \"result\" if state.get(\"count\")==10 else \"classify_input\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_next_node,\n",
    "    {\n",
    "        \"handle_pro\": \"handle_pro\",\n",
    "        \"handle_opp\": \"handle_opp\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_pro\",\n",
    "    check_conv_length,\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_opp\",\n",
    "    check_conv_length,\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.set_entry_point(\"handle_greeting\")\n",
    "workflow.add_edge('handle_greeting', \"handle_pro\")\n",
    "workflow.add_edge('result', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "conversation = app.invoke({'count':0,'history':'Nothing','current_response':''})\n",
    "\n",
    "print(conversation['history'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
